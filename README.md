# Deep-RL-Continuous-Control

A research-grade implementation of state-of-the-art Deep Reinforcement Learning algorithms for continuous control tasks, featuring PPO, SAC, and TD3 with support for both vector and image-based observations.

---

## Overview

This project provides a modular, extensible framework for training and evaluating Deep RL agents on continuous control problems. It implements three modern algorithms—**Proximal Policy Optimization (PPO)**, **Soft Actor-Critic (SAC)**, and **Twin Delayed DDPG (TD3)**—each designed to handle the unique challenges of continuous action spaces. 

The codebase supports: 
- **Vector-based environments** (e.g., LunarLander-v3)
- **Image-based environments** (e.g., CarRacing-v3) with CNN feature extraction
- **Experiment tracking** via Weights & Biases (WandB)
- **Evaluation video recording** for qualitative assessment

---

## Problem Statement

### The Challenge of Continuous Control

Unlike discrete action spaces where agents select from a finite set of actions, **continuous control** requires agents to output real-valued actions (e.g., torque, steering angle, thrust). This introduces several fundamental challenges:

| Challenge | Description |
|-----------|-------------|
| **Infinite Action Space** | Cannot enumerate all actions; requires function approximation to map states to continuous outputs |
| **Exploration Difficulty** | No natural enumeration for ε-greedy; must inject noise or use stochastic policies |
| **High Sensitivity** | Small changes in action values can lead to drastically different outcomes |
| **Credit Assignment** | Precise actions require fine-grained reward signals and stable learning |

This project addresses these challenges through:
- **Stochastic policies** (PPO, SAC) that naturally explore via sampling from learned distributions
- **Deterministic policies with noise** (TD3) that add Gaussian exploration noise during training
- **Stability mechanisms** including target networks, clipped objectives, and entropy regularization

---

## System Pipeline / End-to-End Workflow

### A. High-Level Pipeline Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           TRAINING PIPELINE                                  │
└─────────────────────────────────────────────────────────────────────────────┘

                    ┌──────────────────────────────┐
                    │   Environment Initialization │
                    │  (CarRacing-v3 / LunarLander)│
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │     State Observation        │
                    │  (96x96x3 Image or 8D Vector)│
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    ���   Preprocessing / Encoding   │
                    │  - Image:  CNN → 256D Feature │
                    │  - Vector: Direct Input      │
                    │  - Normalization (0-255→0-1) │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │     Policy / Actor Network   │
                    │  - PPO/SAC: GaussianPolicy   │
                    │  - TD3: DeterministicPolicy  │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │     Action Sampling          │
                    │  - Stochastic:  Sample(μ, σ)  │
                    │  - Deterministic: μ + Noise  │
                    │  - Tanh Squashing → Scaling  │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │     Environment Step         │
                    │  env.step(action) → (s', r)  │
                    │  + ActionRepeat (CarRacing)  │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │  Experience Storage          │
                    │  - Off-policy: ReplayBuffer  │
                    │  - On-policy:  Trajectory     │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │     Policy & Value Updates   │
                    │  - Critic: TD Target + MSE   │
                    │  - Actor: Policy Gradient    │
                    │  - Entropy (SAC) / Clip(PPO) │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │   Target Network Soft Update │
                    │   θ_target ← τθ + (1-τ)θ_tgt │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │   Trained Control Policy     │
                    │   (. pth checkpoint saved)    │
                    └──────────────────────────────┘
```

### B. Step-by-Step Pipeline Explanation

#### 1. Environment Initialization

| Aspect | Details |
|--------|---------|
| **Purpose** | Create the Gymnasium environment and apply necessary wrappers |
| **Input** | Environment name (`CarRacing-v3` or `LunarLander-v3`) |
| **Output** | Wrapped environment ready for interaction |
| **Components** | `gym.make()`, `ActionRepeatWrapper`, `RescaleAction`, `ClipAction` |

**Why necessary:** The raw environment may have edge cases (NaN bounds), require action repetition for stability, or need action rescaling.  The `ActionRepeatWrapper` repeats each action 3 times for CarRacing, reducing decision frequency and smoothing control.

#### 2. State Observation

| Aspect | Details |
|--------|---------|
| **Purpose** | Receive the current state from the environment |
| **Input** | Environment reset or step return |
| **Output** | Raw state tensor (image or vector) |
| **Components** | `env.reset()`, `env.step()` |

**State dimensions:**
- **CarRacing-v3:** 96×96×3 RGB image (uint8, 0-255)
- **LunarLander-v3:** 8-dimensional vector (position, velocity, angle, leg contact)

#### 3. Preprocessing / Encoding

| Aspect | Details |
|--------|---------|
| **Purpose** | Transform raw observations into neural network-compatible features |
| **Input** | Raw state (image or vector) |
| **Output** | Normalized tensor, optionally CNN-encoded |
| **Components** | `CNNFeatureExtractor`, normalization (÷255), `permute(2,0,1)` |

**For image observations:**
1. Permute from HWC to CHW format
2. Normalize pixel values to [0, 1]
3. Pass through 3-layer CNN → 256-dimensional feature vector

**For vector observations:**
1. Direct input to MLP layers (no CNN)

#### 4. Policy / Actor Network

| Aspect | Details |
|--------|---------|
| **Purpose** | Map state features to action distribution parameters or direct actions |
| **Input** | Preprocessed state features |
| **Output** | Action distribution (mean, std) or deterministic action |
| **Components** | `GaussianPolicy`, `DeterministicPolicy` |

**Architecture:**
```
State Features (256D or state_dim)
       │
       ▼
   Linear(256) → ReLU
       │
       ▼
   Linear(256) → ReLU
       │
       ▼
┌──────┴──────┐
│  Stochastic │    │ Deterministic │
│  (PPO/SAC)  │    │    (TD3)      │
├─────────────┤    ├───────────────┤
│ mean, log_σ │    │  Tanh(action) │
└─────────────┘    └───────────────┘
```

#### 5. Action Sampling

| Aspect | Details |
|--------|---------|
| **Purpose** | Generate executable actions with exploration |
| **Input** | Policy network outputs |
| **Output** | Continuous action vector, log probability (for stochastic) |
| **Components** | `Normal` distribution, reparameterization trick, Tanh squashing |

**Stochastic policies (PPO/SAC):**
```
x_t ~ Normal(mean, std)        # Sample from Gaussian
y_t = tanh(x_t)                # Squash to [-1, 1]
action = y_t * scale + bias   # Rescale to action bounds
```

**Deterministic policy (TD3):**
```
action = actor(state)                    # Direct output
action = action + Normal(0, 0.1)         # Add exploration noise
action = clip(action, -max, max)         # Clip to bounds
```

#### 6. Environment Step

| Aspect | Details |
|--------|---------|
| **Purpose** | Execute action and observe transition |
| **Input** | Continuous action vector |
| **Output** | Next state, reward, done flag, info |
| **Components** | `env.step()`, `ActionRepeatWrapper` |

**ActionRepeat mechanism (CarRacing):**
- Repeats same action for 3 consecutive frames
- Accumulates rewards across repetitions
- Terminates early if episode ends

#### 7. Experience Storage

| Aspect | Details |
|--------|---------|
| **Purpose** | Store transitions for learning |
| **Input** | (state, action, reward, next_state, done) |
| **Output** | Filled replay buffer or trajectory |
| **Components** | `ReplayBuffer`, `ImageReplayBuffer`, trajectory lists (PPO) |

**Off-policy (SAC/TD3):**
- Circular buffer with capacity 100,000 transitions
- Uniform random sampling for training batches
- Separate `ImageReplayBuffer` stores uint8 images efficiently

**On-policy (PPO):**
- Episode trajectory stored in lists
- Cleared after each policy update
- Monte Carlo return computation at episode end

#### 8. Policy & Value Updates

| Aspect | Details |
|--------|---------|
| **Purpose** | Improve policy and value estimates using collected experience |
| **Input** | Batch of transitions or complete trajectory |
| **Output** | Updated network parameters, training loss |
| **Components** | Algorithm-specific update rules |

**SAC Update:**
```
1. Sample batch from replay buffer
2. Compute TD target: y = r + γ(min(Q1', Q2') - α·log π)
3. Update critics:  minimize MSE(Q, y)
4. Update actor: minimize α·log π - min(Q1, Q2)
5. Update temperature α:  minimize -α(log π + H_target)
6.  Soft update target networks
```

**TD3 Update:**
```
1. Sample batch from replay buffer
2. Add clipped noise to target actions
3. Compute TD target: y = r + γ·min(Q1', Q2')
4. Update critics: minimize MSE(Q, y)
5. Delayed actor update (every 2 steps)
6. Soft update target networks
```

**PPO Update:**
```
1. Compute discounted returns for trajectory
2. Normalize returns
3. For K epochs:
   - Compute new log probs under current policy
   - Compute probability ratio: r = exp(log π_new - log π_old)
   - Compute clipped surrogate objective
   - Update policy and value network jointly
4. Copy policy to policy_old
```

#### 9. Target Network Soft Update

| Aspect | Details |
|--------|---------|
| **Purpose** | Stabilize learning by slowly tracking main network |
| **Input** | Main network parameters, target network parameters |
| **Output** | Updated target network parameters |
| **Components** | Polyak averaging with τ=0.005 |

```python
θ_target = τ * θ_main + (1 - τ) * θ_target
```

**Why necessary:** Hard updates cause instability; soft updates provide smooth, stable TD targets.

#### 10.  Trained Policy Checkpoint

| Aspect | Details |
|--------|---------|
| **Purpose** | Save trained policy for inference or further training |
| **Input** | Policy network state dict |
| **Output** | `.pth` checkpoint file |
| **Components** | `torch.save()`, model directory |

---

## Key Features

- **Multi-Algorithm Support:** PPO, SAC, and TD3 implementations in a unified framework
- **Observation Flexibility:** Automatic detection and handling of image vs.  vector observations
- **CNN Feature Extraction:** 3-layer convolutional encoder for 96×96 image inputs
- **Robust Action Handling:** NaN/Inf sanitization for Gymnasium action space bounds
- **ActionRepeat Wrapper:** Frame skipping for more stable control in CarRacing
- **Experiment Tracking:** Full WandB integration with metrics, losses, and evaluation videos
- **Modular Architecture:** Clean separation of agents, networks, and replay buffers

---

## Algorithms & RL Techniques

### Algorithm Comparison

| Property | PPO | SAC | TD3 |
|----------|-----|-----|-----|
| **Learning Type** | On-policy | Off-policy | Off-policy |
| **Policy Type** | Stochastic | Stochastic | Deterministic |
| **Exploration** | Policy entropy | Entropy regularization | Gaussian noise |
| **Critic Architecture** | Value network V(s) | Twin Q-networks | Twin Q-networks |
| **Key Innovation** | Clipped surrogate objective | Maximum entropy RL | Delayed policy updates |
| **Sample Efficiency** | Lower | Higher | Higher |
| **Stability** | High (clipping) | High (entropy + targets) | High (twin Q + delay) |

### Exploration Strategies

**Stochastic Policies (PPO/SAC):**
- Sample actions from learned Gaussian distribution
- Reparameterization trick enables gradient flow through sampling
- Tanh squashing bounds actions while preserving gradients
- SAC additionally maximizes entropy for exploration

**Deterministic Policy (TD3):**
- Add Gaussian noise (σ=0.1) to actions during training
- Target policy smoothing:  add clipped noise (σ=0.2, clip=0.5) to target actions
- Evaluation uses deterministic mean action

### Stability Mechanisms

| Mechanism | Algorithm | Purpose |
|-----------|-----------|---------|
| Target Networks | SAC, TD3 | Stable TD targets via Polyak averaging |
| Twin Q-Networks | SAC, TD3 | Reduce overestimation bias |
| Delayed Policy Updates | TD3 | Update actor less frequently than critic |
| Clipped Objective | PPO | Prevent destructive policy updates |
| Entropy Regularization | SAC | Encourage exploration, prevent premature convergence |
| Temperature Tuning | SAC | Automatic entropy coefficient adaptation |

---

## Environment(s)

### CarRacing-v3 (Image-Based)

| Property | Value |
|----------|-------|
| **Observation Space** | Box(0, 255, (96, 96, 3), uint8) |
| **Action Space** | Box(-1, 1, (3,), float32) — [steering, gas, brake] |
| **Reward** | +1000/N per tile visited, -0.1 per frame |
| **Termination** | All tiles visited or 1000 frames |

### LunarLander-v3 (Vector-Based)

| Property | Value |
|----------|-------|
| **Observation Space** | Box(-inf, inf, (8,), float32) |
| **Action Space** | Box(-1, 1, (2,), float32) — [main engine, side engine] |
| **Reward** | Landing:  +100 to +140, Crash: -100, Fuel: -0.3/frame |
| **Termination** | Landed, crashed, or left viewport |

---

## Training & Evaluation

### Training Loop

1. Initialize environment and agent
2. For each timestep:
   - Select action (with exploration)
   - Execute in environment
   - Store transition
   - Update networks (step-level for SAC/TD3, episode-level for PPO)
3. Periodic evaluation with video recording
4. Save checkpoints at intervals and final model

### Evaluation Protocol

- Deterministic action selection (no exploration noise)
- Full episode rollout with video recording
- Metrics logged: episode reward, duration
- Videos uploaded to WandB

### Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 3e-4 | Adam optimizer for all networks |
| Batch Size | 128 | Transitions per update (off-policy) |
| Buffer Size | 100,000 | Replay buffer capacity |
| Gamma (γ) | 0.99 | Discount factor |
| Tau (τ) | 0.005 | Target network update rate |
| PPO Clip (ε) | 0.2 | Surrogate objective clip range |
| PPO Epochs | 10 | Updates per trajectory |
| SAC Alpha (α) | 0.2 (auto-tuned) | Entropy temperature |
| TD3 Policy Noise | 0.2 | Target policy smoothing noise |
| TD3 Noise Clip | 0.5 | Noise clipping range |
| TD3 Policy Freq | 2 | Actor update frequency |

---

## Installation & Setup

### System Dependencies

```bash
# Ubuntu / Debian
sudo apt-get update
sudo apt-get install -y xvfb ffmpeg swig

# macOS (with Homebrew)
brew install swig ffmpeg
```

### Python Environment

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
.\venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Or install manually
pip install torch>=2.0.0 "gymnasium[box2d]>=0.29.0" numpy>=1.24.0 wandb>=0.15.0 moviepy>=1.0.0 imageio
```

### WandB Setup

```bash
wandb login
# Enter your API key when prompted
```

---

## Usage Examples

### Basic Training

```bash
# Train SAC on CarRacing
python train.py --model SAC --env CarRacing-v3

# Train PPO on LunarLander
python train.py --model PPO --env LunarLander-v3

# Train TD3 with custom timesteps
python train. py --model TD3 --env CarRacing-v3 --timesteps 500000
```

### Full Command Reference

```bash
python train.py \
    --model SAC \                    # Algorithm:  SAC, TD3, or PPO
    --env CarRacing-v3 \             # Environment: CarRacing-v3 or LunarLander-v3
    --timesteps 200000 \             # Total training steps
    --wandb_project MyProject        # WandB project name
```

### Loading and Evaluating a Trained Model

```python
import torch
from agents.SAC import SACAgent
import gymnasium as gym

# Initialize environment and agent
env = gym.make("LunarLander-v3", continuous=True, render_mode="human")
config = {"gamma": 0.99, "learning_rate": 3e-4, "buffer_size": 100000}
agent = SACAgent(
    state_dim=8,
    action_dim=2,
    action_space=env.action_space,
    config=config,
    use_cnn=False
)

# Load trained policy
agent.policy. load_state_dict(torch.load("saved_models/SAC_LunarLander-v3_final.pth"))

# Run evaluation episode
state, _ = env.reset()
done = False
total_reward = 0

while not done: 
    action = agent.select_action(state, evaluate=True)
    state, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    total_reward += reward

print(f"Episode Reward: {total_reward}")
env.close()
```

---

## Example Episode / Trajectory

### LunarLander-v3 Sample Trajectory

```
Step    State (x, y, vx, vy, θ, ω, leg_L, leg_R)       Action (main, side)    Reward
──────────────────────────────────────────────────────────────────────────��─────────
0       [0.00, 1.40, 0.00, 0.00, 0.00, 0.00, 0, 0]     [0.00, 0.00]          0.00
10      [0.02, 1.35, 0.01, -0.05, 0.01, 0.00, 0, 0]    [0.50, 0.10]         -0.03
50      [0.15, 0.80, 0.03, -0.20, 0.05, 0.02, 0, 0]    [0.80, -0.20]        -0.15
100     [0.10, 0.40, 0.01, -0.15, 0.02, 0.00, 0, 0]    [0.60, 0.05]         -0.30
150     [0.05, 0.10, 0.00, -0.05, 0.01, 0.00, 1, 1]    [0.20, 0.00]         +100
────────────────────────────────────────────────────────────────────────────────────
Total Episode Reward: ~+140 (successful landing)
```

### CarRacing-v3 Action Interpretation

```
Action Vector: [steering, gas, brake]
  - steering: -1 (full left) to +1 (full right)
  - gas:       -1 (no gas)    to +1 (full throttle)
  - brake:    -1 (no brake)  to +1 (full brake)

Example smooth turn:
  [0.3, 0.6, 0.0]  →  Slight right turn, moderate acceleration, no braking
```

---

## Project Structure

```
Deep-RL-Continuous-Control/
├── agents/                      # RL Agent Implementations
│   ├── PPO. py                   # Proximal Policy Optimization
│   ├── SAC.py                   # Soft Actor-Critic
│   └── TD3.py                   # Twin Delayed DDPG
├── utils/                       # Utilities and Components
│   ├── NNArch.py                # Neural Network Architectures
│   │   ├── CNNFeatureExtractor  # 3-layer CNN for images
│   │   ├── GaussianPolicy       # Stochastic policy (PPO/SAC)
│   │   ├── DeterministicPolicy  # Deterministic policy (TD3)
│   │   ├── DoubleQNetwork       # Twin Q-networks (SAC/TD3)
│   │   └── ValueNetwork         # State value function (PPO)
│   ├── ReplayBuffer.py          # Off-policy buffer for vectors
│   └── ImageReplayBuffer.py     # Off-policy buffer for images
├── train.py                     # Main training script
├── upload_to_hf.py              # Hugging Face Hub upload utility
├── requirements.txt             # Python dependencies
├── saved_models/                # Trained model checkpoints (generated)
├── videos/                      # Evaluation recordings (generated)
└── README.md                    # This file
```

---

## Limitations

| Limitation | Description |
|------------|-------------|
| **Environment Scope** | Currently supports only CarRacing-v3 and LunarLander-v3 |
| **Single-Agent Only** | No multi-agent or distributed training support |
| **No Prioritized Replay** | Uniform sampling may slow learning on hard transitions |
| **Fixed Hyperparameters** | Limited hyperparameter tuning interface |
| **No Model Saving During Training** | Only saves at evaluation intervals |
| **GPU Memory** | Image replay buffer can consume significant memory |
| **No Curriculum Learning** | Fixed environment difficulty throughout training |

---

## Future Improvements

- [ ] **Prioritized Experience Replay** for SAC/TD3
- [ ] **Generalized Advantage Estimation (GAE)** for PPO
- [ ] **Multi-environment support** (MuJoCo, DMControl)
- [ ] **Distributed training** with Ray or IMPALA
- [ ] **Hyperparameter sweeps** via WandB
- [ ] **Model-based components** (world models, Dreamer)
- [ ] **Offline RL support** (CQL, IQL)
- [ ] **Frame stacking** for temporal information
- [ ] **Mixed precision training** for faster image-based learning
- [ ] **Inference-only mode** with pre-trained model zoo

---

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/NewAlgorithm`)
3. Implement changes with tests
4. Ensure code follows existing patterns
5. Commit with descriptive messages
6. Push to your branch and open a Pull Request

### Code Style
- Follow PEP 8 conventions
- Document new classes and methods
- Add type hints where appropriate

---

## License

This project is open-source and available under the **MIT License**.

```
MIT License

Copyright (c) 2024

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions: 

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
